{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU datasets transformers sentence-transformers git+https://git@github.com/pinecone-io/pinecone-python-client.git#egg=pinecone-client[grpc] git+https://github.com/naver/splade.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('/kaggle/input/constitution-of-india/Constitution Of India.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laws = list(data['Articles'])\n",
    "for i in range(len(laws)):\n",
    "    laws[i] = 'Article ' + laws[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "# for record in pubmed:\n",
    "#     chunks = chunker(record['context']['contexts'])\n",
    "#     for i, context in enumerate(chunks):\n",
    "#         data.append({\n",
    "#             'id': f\"{record['pubid']}-{i}\",\n",
    "#             'context': context\n",
    "#         })\n",
    "for i, context in enumerate(laws):\n",
    "        data.append({\n",
    "            'id': f\"{i}\",\n",
    "            'context': context\n",
    "        })\n",
    "\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# check device being run on\n",
    "if device != 'cuda':\n",
    "    print(\"==========\\n\"+\n",
    "          \"WARNING: You are not running on GPU so this may be slow.\\n\"+\n",
    "          \"If on Google Colab, go to top menu > Runtime > Change \"+\n",
    "          \"runtime type > Hardware accelerator > 'GPU' and rerun \"+\n",
    "          \"the notebook.\\n==========\")\n",
    "\n",
    "dense_model = SentenceTransformer(\n",
    "    'msmarco-bert-base-dot-v5',\n",
    "    device=device\n",
    ")\n",
    "dense_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splade.models.transformer_rep import Splade\n",
    "\n",
    "sparse_model_id = 'naver/splade-cocondenser-ensembledistil'\n",
    "\n",
    "sparse_model = Splade(sparse_model_id, agg='max')\n",
    "sparse_model.to(device)  # move to GPU if possible\n",
    "sparse_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(sparse_model_id)\n",
    "\n",
    "tokens = tokenizer(data[0]['context'], return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    sparse_emb = sparse_model(\n",
    "        d_kwargs=tokens.to(device)\n",
    "    )['d_rep'].squeeze()\n",
    "sparse_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = sparse_emb.nonzero().squeeze().cpu().tolist()\n",
    "values = sparse_emb[indices].cpu().tolist()\n",
    "sparse = {'indices': indices, 'values': values}\n",
    "sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2token = {idx: token for token, idx in tokenizer.get_vocab().items()}\n",
    "\n",
    "sparse_dict_tokens = {\n",
    "    idx2token[idx]: round(weight, 2) for idx, weight in zip(indices, values)\n",
    "}\n",
    "# sort so we can see most relevant tokens first\n",
    "sparse_dict_tokens = {\n",
    "    k: v for k, v in sorted(\n",
    "        sparse_dict_tokens.items(),\n",
    "        key=lambda item: item[1],\n",
    "        reverse=True\n",
    "    )\n",
    "}\n",
    "sparse_dict_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "\n",
    "def builder(records: list):\n",
    "    ids = [x['id'] for x in records]\n",
    "    contexts = [x['context'] for x in records]\n",
    "    # create dense vecs\n",
    "    dense_vecs = dense_model.encode(contexts).tolist()\n",
    "    # create sparse vecs\n",
    "    input_ids = tokenizer(\n",
    "        contexts, return_tensors='pt',\n",
    "        padding=True, truncation=True\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        sparse_vecs = sparse_model(\n",
    "            d_kwargs=input_ids.to(device)\n",
    "        )['d_rep'].squeeze()\n",
    "    # convert to upsert format\n",
    "    upserts = []\n",
    "    for _id, dense_vec, sparse_vec, context in zip(ids, dense_vecs, sparse_vecs, contexts):\n",
    "        # extract columns where there are non-zero weights\n",
    "        indices = sparse_vec.nonzero().squeeze().cpu().tolist()  # positions\n",
    "        values = sparse_vec[indices].cpu().tolist()  # weights/scores\n",
    "        # build sparse values dictionary\n",
    "        sparse_values = {\n",
    "            \"indices\": indices,\n",
    "            \"values\": values\n",
    "        }\n",
    "        # build metadata struct\n",
    "        metadata = {'context': context}\n",
    "        # append all to upserts list as pinecone.Vector (or GRPCVector)\n",
    "        upserts.append({\n",
    "            'id': _id,\n",
    "            'values': dense_vec,\n",
    "            'sparse_values': sparse_values,\n",
    "            'metadata': metadata\n",
    "        })\n",
    "    return upserts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "pinecone.init(\n",
    "    api_key=\"f112db94-1b02-44ec-b1d7-a4cf165fad28\",  # app.pinecone.io\n",
    "    environment=\"us-east1-gcp\"  # next to api key in console\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'law-gpt'\n",
    "\n",
    "pinecone.create_index(\n",
    "    index_name,\n",
    "    dimension=768,\n",
    "    metric=\"dotproduct\",\n",
    "    pod_type=\"s1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pinecone.GRPCIndex(index_name)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    # extract batch of data\n",
    "    i_end = min(i+batch_size, len(data))\n",
    "    batch = data[i:i_end]\n",
    "    # pass data to builder and upsert\n",
    "    index.upsert(builder(data[i:i+batch_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data), index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text: str):\n",
    "    # create dense vec\n",
    "    dense_vec = dense_model.encode(text).tolist()\n",
    "    # create sparse vec\n",
    "    input_ids = tokenizer(text, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        sparse_vec = sparse_model(\n",
    "            d_kwargs=input_ids.to(device)\n",
    "        )['d_rep'].squeeze()\n",
    "    # convert to dictionary format\n",
    "    indices = sparse_vec.nonzero().squeeze().cpu().tolist()\n",
    "    values = sparse_vec[indices].cpu().tolist()\n",
    "    sparse_dict = {\"indices\": indices, \"values\": values}\n",
    "    # return vecs\n",
    "    return dense_vec, sparse_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who is the Chief Justice of High Court when the seat is vacant\"\n",
    "dense, sparse = encode(query)\n",
    "# query\n",
    "xc = index.query(\n",
    "    vector=dense,\n",
    "    sparse_vector=sparse,\n",
    "    top_k=2,  # how many results to return\n",
    "    include_metadata=True\n",
    ")\n",
    "xc"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
